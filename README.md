<p align="center">
  <img src="resources/PHOENYX-1-logo-recortado.png" alt="Project Logo"/>
</p>


> ### **ğŸš€ From Mars to the Lab:**
>
> ğŸ¤– **Meet Phoenyx I**, an autonomous exploration rover inspired by **NASAâ€™s designs** â€” bringing **cutting-edge robotics** from the lab to Mars-like terrains.
>
> ğŸ§  Powered by **AI**, **computer vision**, **SLAM**, and **ROS 2**, Phoenyx I is engineered to handle both **Mars-analogue exploration** and **complex real-world robotics challenges**.
> 
> ğŸ† **Winner of Best Overall Rover & Design Excellence** at **Sener-CEA's Bot Talent competition**, itâ€™s not just a prototype, itâ€™s a **proven platform** for **autonomous field robotics**.

---

 ## ğŸ‘€ Watch it in action 
<table>
  <tr>
    <td>
      <img src="resources/Phoenyx-I.jpg" alt="Project Logo" width="800" />
    </td>
    <td>
      <ul>
        <li><a href="https://youtube.com/shorts/iHNUQLfxfGA?feature=share">ğŸ“¹ Perception Task</a></li>
        <li><a href="https://youtu.be/W66J1JEbJms">ğŸ“¹ Control Task</a></li>
        <li><a href="https://youtu.be/kr9DZYW80oY">ğŸ“¹ Guided Task</a></li>
        <li><a href="https://www.instagram.com/pucra.upc/">ğŸ“· Behind the Scenes</a></li>
        <li><a href="https://www.lavanguardia.com/launi/20250515/10686074/doble-victoria-equipo-upc-competicion-diseno-programacion-robots-superar-misiones-nasa.html">ğŸ“ˆ Article in Spanish</a></li>
        <li><a href="https://www.group.sener/noticias/la-universidad-politecnica-de-catalunya-gana-la-final-de-sener-ceas-bot-talent-el-concurso-de-robotica-de-la-fundacion-sener-y-el-comite-espanol-de-automatica/">ğŸ“ˆ Article in Sener's web</a></li>
      </ul>
    </td>
  </tr>
</table>

## ğŸ“‚ What's Inside This Repository?

This repository includes the full **source code**, **ROS 2 packages**, and **system configurations** for **Phoenyx I**, the award-winning autonomous rover engineered by undergraduate students from **PUCRA**, the robotics association at the **Polytechnic University of Catalonia**. 

Base on [NASA JPL Open Source Rover](https://github.com/nasa-jpl/open-source-rover),this project enhances the base platform with a robust autonomy stack:
- ğŸ” **Real-time perception** (color, digit recognition & ArUco makers) 
- ğŸ—ºï¸ **SLAM & LiDAR-based navigation**.
- ğŸ§­ **Global and local path planning** with obstacle avoidance.
- ğŸ§  **Finite State Machine** (FSM) architecture in ROS 2.
- âš™ï¸ **Optimized performance on Raspberry Pi 4B.**

Phoenyx I demonstrates how high-performance autonomy can be achieved using **accessible hardware, efficient algorithms, and a ROS 2 architecture**, serving as a scalable platform for education, research, and field robotics experimentation.

## ğŸ“¦ Jump to:

- [ğŸ¯ Competition Challenges Overview](#-competition-challenge-overview)
- [ğŸ› ï¸ Development Environment](#%EF%B8%8F-development-environment)
- [ğŸ“‚ Repo Structure](#-repository-structure)
- [ğŸš¦ How to Run](#-how-to-run-the-system) 
- [ğŸ Competition Results](#-competition-results)
- [ğŸ¤ Want to Collaborate?](#-want-to-collaborate)
- [ğŸŒ Join & Follow Us](#-join--follow-us)

## ğŸ¯ Competition Challenge Overview
The **SENER-CEA's Bot Talent** competition features a serie of challenges focused on **AMRs (Autonomus Mobile Robots)**, in which universities from across Spain compete by completing some tasks using an open source Rover. Our team successfully tackled all the proposed challenges:
- **[ğŸ” Perception Task](#-perception-task)**: Visual marker classification.
- **[ğŸ›£ï¸ Control Task](#-control-task)**: Corridor navigation via LiDAR
- **[ğŸ“Guidance Task](#-guidance-task)**: Localization + waypoint following using ArUco.
- **ğŸ§©Final Challenge**:  Full autonomous mission combining all above tasks.
  
### ğŸ” Perception Task

Given the **Raspberry Pi 4B's** limited **4GB memory**, we opted for a lightweight **k-Nearest Neighbors (kNN)** classifier instead of deep CNNs. This approach was combined with classical **computer vision technique** such as **morphological treatments**, **custom filtering**, and **image preprocessing** to isolate digits from their background. Additionally, a **stadistic analysis** was employed for accurate **color detection**. This combination enabled **fast** and **reliable digit and color recognition** with **minimal computational overhead**.

### ğŸ›£ï¸ Control Task
In this challenge, the robot had to **autonomously navigate narrow hallways using only 2D LiDAR**, without predefined maps or waypoints. We developed a custom ROS 2 node, `linea_media.py`, that combines **local perception** and **global goal planning** via Nav2.

The node continuously processes the LiDAR scan data (-80Âº to 80Âº), **identifies** the most **open path**, **projects the best direction** to the global `map` frame, and **publish a `PoseStamped` goal** to Nav2.

Optimized for the **Raspberry Pi 4B**, it leverages lightweight methods like **block averaging, polar gap detection, and adaptive filtering** to ensure **real-time, robust, and safe operation**, validated in both simulation and on the competition floor.

### ğŸ“ Guidance Task 

In this challenge, the robot had to **localize itself using ArUco markers** markers from an unknown starting position. This was handled by `brain.py` (a finite state machine (FSM) that coordinates all nodes) and `localizacion_aruco.py`, which scan **ArUco markers** and triggers an **odometry reset** via manual **frame transformations**.

To ensure safty, the system loads a predifined `map` to keep the robot within fiel boundaries.

## ğŸ› ï¸ Development Environment

### System Requirements 

- **OS:** Ubuntu 22.04 LTS
- **ROS 2:** Humble Hawksbill
- **Hardware:**
  - Raspberry Pi 4B (4 GB RAM)
  - YDLidar X4
  - Orbbec AstraPro Plus (RGB-D Camera)
  - Adafruit BNO055 IMU
  - Arduino (for Neopixel Led control)
  - INA260n (for battery monitor)
  - LiPo battery 4S
  - Joystick controller
  - Emergency button 
  - Mechanical components based on the [NASA JPL Open Source Rover](https://github.com/nasa-jpl/open-source-rover)
  
### âš  Dependencies

- `slam_toolbox` â€“ Real-time SLAM and map generation.
- `nav2` â€“ Navigation and planning stack.
- `rviz2`, `gazebo_ros` â€“ Simulation and visualization
- `rclpy`, `geometry_msgs`, `sensor_msgs`, `tf2_ros` â€“ ROS core communication.
- `OpenCV`, `numpy` â€“ Computer vision and data ops.
- `joy`, `teleop_twist_joy` â€“ Manual joystick control.
- `scickit-learn`- AI and image recognition

## ğŸ“ Repository Structure

```bash
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ osr_bringup/            # Basic launch files and configuration for the OSR
â”‚   â”œâ”€â”€ osr_control/            
â”‚   â”œâ”€â”€ osr_control_challenge/  # Code for the control task
â”‚   â”œâ”€â”€ osr_gazebo/             # Simulation environment for Gazebo
â”‚   â”œâ”€â”€ osr_interfaces/         # Custom ROS messages
â”‚   â”œâ”€â”€ percepcion/             # Image recognition, color and digit detection
â”‚   â”œâ”€â”€ phoenyx_nodes/          # Multiple nodes for different tasks and applications
â”‚   â”œâ”€â”€ planificador/           # Package for custom launch and YAML configuration
â”‚   â”œâ”€â”€ guiado/                 # SLAM-based localization and waypoint navigation
â”‚   â”œâ”€â”€ control/                
â”‚   â”œâ”€â”€ datos/                  
â”‚   â””â”€â”€ final/                  
â”‚
â”œâ”€â”€ LICENSE.md
â”œâ”€â”€ JPL_NASA_LICENCE.txt
â””â”€â”€ README.md

```

## ğŸš¦ How to Run the System

### ğŸ§ª In Simulation 

#### Empty Worlds
```bash
# Launch an empty world on gazebo with the rover model
ros2 launch osr_gazebo empty_world.launch.py

# Launch an empty world on gazebo with the simplified model of the rover model
ros2 launch osr_gazebo empty_world_simplified.launch.py

# Launch rviz with the rover model
ros2 launch osr_gazebo rviz.launch.py

# Launch rviz with the simplified model of the rover model
ros2 launch osr_gazebo rviz_simplified.launch.py 
```

### Move With a Controller
```bash
# With an open gazebo world with the rover model you can use a controller to move it
ros2 launch osr_bringup joystick_launch.py 
```

#### Control Task (Without Nav2)
This mode is used to evaluate the roverâ€™s control and navigation behavior inside a **maze environment**.
Three maze worlds are provided, each with different layouts and difficulty levels.

Available maze worlds:
- maze_1.world â€” basic layout, suitable for tuning and testing.
- maze_2.world â€” intermediate complexity.
- maze_3.world â€” advanced maze for full control evaluation.

```bash
# Terminal 1 - Launch the simulation world with the desired maze
ros2 launch osr_gazebo maze_simulation.launch.py maze:=maze_1.world
# or
ros2 launch osr_gazebo maze_simulation.launch.py maze:=maze_2.world
# or
ros2 launch osr_gazebo maze_simulation.launch.py maze:=maze_3.world

# Terminal 2 - Run the control challenge node
ros2 run osr_control_challenge maze_navigation
```

#### Guidance Task

```bash
# Terminal 1 - Launch simulation world
ros2 launch osr_gazebo circuito_arucos.launch.py

# Terminal 2 - Launch SLAM
ros2 launch slam_toolbox online_async_launch.py use_sim_time:=true

# Terminal 3 - Launch Nav2
ros2 launch planificador planificador_launch.py use_sim_time:=true

# Terminal 4 - Launch Brain
ros2 run guiado brain_guiado.py use_sim_time:=true

# Terminal 5 - Publish a true on topic /aruco_scan
ros2 topic pub --once /aruco_scan std_msgs/Bool "{data: true}"

```

### ğŸ¤– On Real Robot 
#### For percepcion task
``` bash
ros2 launch prueba2_percepcion.launch.py 
```
#### For control task 
```bash
# Terminal 1 
ros2 launch control control.launch.py

# Terminal 2
ros2 launch control planificador.launch.py
```
#### For guiado task 
```bash
ros2 launch guiado guiado.launch.py
```

> [!note]
> Press A on the joystick to start autonomous mode `/joy` topic.
>
>  See [Orbbec ROS 2 README](https://github.com/PUCRA/Phoenyx/tree/main/OrbbecSDK_ROS2) for camera setup.

## ğŸ Competition Results

- ğŸ¥‡ **First Place Overall â€“ Bot Talent 2025**  
- ğŸ§  **Awarded for Best Robot Design**   

## License

This main project, including all contributions by PUCRA and its collaborators, is licensed under the **[MIT License](LICENSE)**.

Parts of this project derived from the work of the **Jet Propulsion Laboratory (JPL) of NASA** are covered by the **[Apache License, Version 2.0](JPL_NASA_License.txt)**. Please refer to the `JPL_NASA_License.txt` file for full terms and original attributions.

## ğŸ¤ Want to Collaborate?

If you're interested in contributing code, improving documentation, or developing new features, feel free to check our [CONTRIBUTING](.github/CONTRIBUTING.md) page!

## ğŸŒ Join & Follow Us

Stay connected with PUCRA and follow our journey:

[![email](https://img.shields.io/badge/Email-D14836?logo=gmail&logoColor=white)](mailto:pucra.eebe@upc.edu) 
[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?logo=linkedin&logoColor=white)](https://www.linkedin.com/company/pucra-upcc/posts/?feedView=all)
[![Instagram](https://img.shields.io/badge/Instagram-%23E4405F.svg?logo=Instagram&logoColor=white)](https://www.instagram.com/pucra.upc/)
[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?logo=YouTube&logoColor=white)](https://www.youtube.com/@pucraupc) 

<p align="center">
  <img src="resources/logo.png" alt="Project Logo"/>
</p>

